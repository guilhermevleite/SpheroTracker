# -*- coding: utf-8 -*-
"""Segmentation_Statistical_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4oXo_xTgulETvInypWAQi0TsBvzm0aI

# TODO
- [ ] Fill holes segmentation
- [ ] Organizar blocos da análise estatística
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install statannotations

"""## Imports"""

from pathlib import Path
import cv2 as cv
import numpy as np
import math
import csv
import pandas as pd
from scipy.stats import tukey_hsd
import scipy.stats as stats
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly
pd.options.plotting.backend = 'plotly'

from tqdm import tqdm
from itertools import combinations
from statannotations.Annotator import Annotator

"""# Debug Helper

**TODO** Deletar depois

*Estou usando esse código para descobrir o problema com o threshold dando erro*
"""

from google.colab.patches import cv2_imshow

def debug_img(image_lst, color):

    iterator = iter(image_lst)
    horizontal = next(iterator)

    if len(horizontal.shape) == 2:
        horizontal = cv.cvtColor(horizontal, cv.COLOR_GRAY2BGR)

    for image in iterator:
        if len(image.shape) == 2:
            image = cv.cvtColor(image, cv.COLOR_GRAY2BGR)

        print(horizontal.shape, image.shape)
        horizontal = cv.hconcat([horizontal, image])

        cv2_imshow(horizontal)


def colorize_connected_components(labels):
    label_hue = np.uint8(179*labels/np.max(labels))
    blank_ch = 255*np.ones_like(label_hue)
    labeled_img = cv.merge([label_hue, blank_ch, blank_ch])

    labeled_img = cv.cvtColor(labeled_img, cv.COLOR_HSV2BGR)

    labeled_img[label_hue==0] = 0

    return labeled_img

"""## Global Variables"""

# TODO : ILUSTRAR ISSO COM FIGURA OU APONTAR NO EXEMPLO
# Path to the folder of the DAYS folder.

# String GUI 🥛
CONFIG_INPUT = "/content/drive/MyDrive/spheroid_bia_segmentation/grupo_2"
# String BIA
# CONFIG_INPUT = "/content/drive/MyDrive/Mestrado/papers_em_desenvolvimento/segmentation_analysis/images_organized/grupo_2" #podemos deixar um caminho comum
# Size of the output image, 512 means the output image will be 512x512 in size.
CONFIG_OUTPUT_SIZE = 512

# Scale of one pixel, in our case one pixel is 0.8333um (micro-meters), the unit is irrelevant.
PIXEL_SCALE = 7.0175


# Number of compounds (TreaTmenT) being tested, DO NOT include control in this counter
TTT_COUNT = 3

"""## Image Loader Functions"""

def image_pre_loading(img):
    '''
    Sanity check on recently loaded pictures. If the image failed to load for some reason, a black image is returned.
    The output image is also resized to the desired output size, specified in global variable CONFIG_OUTPUT_SIZE
    '''
    if img is None:
        return np.zeros((CONFIG_OUTPUT_SIZE, CONFIG_OUTPUT_SIZE), dtype="uint8")

    return cv.resize(img, (CONFIG_OUTPUT_SIZE, CONFIG_OUTPUT_SIZE))


def get_samples_name_list(folder_list: Path) -> list:
    '''
    Returns a list with the path of all sample image, using the DAY folder as reference.

    sampling_path: Path to the folder containing the images folder of the sampling (day).
    '''

    path_first_folder = folder_list[0]
    sample_name_list: list = []

    # print('FOLDER ', path_first_folder)
    for sample in sorted((path_first_folder / 'images').glob('*')):
        sample_name_list.append(sample)

    return sample_name_list


def get_sample_images(sample_name: str, sample_folder_list: Path) -> list:
    '''
    Loads the sample image for all listed folders, returns a list with all sample images.

    sample_name: Name of the target sample.
    sample_folder_list: List of the folders to load the sample.
    '''

    image_lst = []

    for sample_folder in sample_folder_list:
        if not sample_folder.is_dir():
            continue

        image_path = str(sample_folder / "images" / sample_name)

        if Path(image_path).exists():
            img = cv.imread(image_path, 0)
        else:
            print('WARNING, not loaded', image_path)
            img = None

        img = image_pre_loading(img)
        image_lst.append(img)

    return image_lst

"""## Segmentation Functions"""

def threshold(image):
    '''
    Applies the OTSU threshold to this image.
    Returns the resulting mask.
    '''

    img = cv.bitwise_not(image)
    _, thresh = cv.threshold(img, 0, 255, cv.THRESH_OTSU)

    return thresh

def post_processing_threshold(thresh):
    '''
    Executes some post_processing steps on top of the thresholded image.
    This steps are erode operations, to vanish with small regions created
    by the thresholding algorithm.
    Returns the resulting image.
    '''
    kernel = cv.getStructuringElement(cv.MORPH_CROSS, (3,3))

    thresh = cv.erode(thresh, kernel)
    thresh = cv.erode(thresh, kernel)
    thresh = cv.erode(thresh, kernel)

    # The threshold generated some black spots in the spheroid region, this should eliminate those.
    # thresh = cv.resize(thresh, (CONFIG_OUTPUT_SIZE//2, CONFIG_OUTPUT_SIZE//2))
    thresh = cv.dilate(thresh, kernel)
    #thresh = cv.dilate(thresh, kernel)
    # thresh = cv.resize(thresh, (CONFIG_OUTPUT_SIZE, CONFIG_OUTPUT_SIZE))

    # thresh[thresh > 0] = 255

    return thresh

"""## Connected Components Functions






"""

def find_largest_region(stats_list):
    '''
    Finds the largest region in the stats list, and returns
    its index.
    stats_list: List returned by the connectedcomponent
    analysis, again, the largest region is assumed to be the
    spheroid region.
    '''

    smallest = math.inf
    spheroid_label = -1

    for idx, region in enumerate(stats_list):
        # Index [4] is the area value
        if region[4] > stats_list[spheroid_label][4] and idx > 0 and region[0] != 0:

            spheroid_label = idx

    return spheroid_label

def get_spheroid_mask_from_thresh(segmented_list) -> tuple:
    '''
    Returns two lists, the first list containing the
    spheroid region segmentation, and a list with their
    areas.
    segmented_list: Is a list of the threshold results, with
    many connected regions. We assume that the largest
    connected region is also the spheroid region.
    '''

    mask_list = []
    area_list = []
    for idx, spheroid in enumerate(segmented_list):

        _, labels, stats, centroids = cv.connectedComponentsWithStats(spheroid)
        possible_spheroid_region_label = find_largest_region(stats)

        colorize_connected_components(labels)

        gray_image = None
        # Image is already in grayscale
        if len(spheroid.shape) < 3:
            gray_image = np.copy(spheroid)
        # Image needs to be converted to grayscale
        else:
            gray_image = cv.cvtColor(spheroid, cv.COLOR_BGR2GRAY)

        # Quantize the image
        gray_image[labels == possible_spheroid_region_label] = 255
        gray_image[labels != possible_spheroid_region_label] = 0

        mask_list.append(gray_image)
        area_list.append(stats[possible_spheroid_region_label][4])
        # print(stats[possible_spheroid_region_label][4])

        # print('\tDay:', idx)
        # if idx == 3:
        #     print(idx)
        #     color_coded = colorize_connected_components(labels)
        #     debug_img([spheroid, color_coded], 0)

    return mask_list, area_list

"""## Output Functions"""

def save_sample_segmentation_render(input_folder,
                                    full_path,
                                  mask_list,
                                  area_list):
    '''
    Saves the sample segmentation render results into the
    file system.
    The target folder is created next to the <images> folder,
    and is called masks.
    '''

    days = sorted(Path(input_folder).glob('*'))
    for idx, mask in enumerate(mask_list):
        if not days[idx].is_dir():
            continue

        file_name = str(full_path.name).replace('.tif', '.png')

        mask_dir = days[idx] / 'masks'
        mask_dir.mkdir(parents=True, exist_ok=True)

        cv.imwrite(str(mask_dir / file_name), mask)


def save_segmentation_csv(folder, well_list, area_list):

    with open(folder + '/areas.csv', 'w') as f:
        write = csv.writer(f)

        header = ['time', 'sample', 'area', 'experiment']
        write.writerow(header)

        for w_idx, well in enumerate(well_list):
            name = (well.name).split('.')[0]
            for day, area in enumerate(area_list[w_idx]):
                experiment = 'control'
                if 'test' in name:
                    string = name.split('_')
                    experiment = f'{string[0]}_{string[1]}'
                if area == 0:
                    row = [day, name, 'NaN', experiment]
                else:
                    row = [day, name, f'{(area * PIXEL_SCALE):.3f}', experiment]
                write.writerow(row)

"""## Segmentation Steps"""

def segmentation_method(image_list):
    '''
    Segments all images in image_list.
    Returns a list with the images masks.
    '''
    mask_list = []

    for idx, image in enumerate(image_list):
        simple_threshold = threshold(image)
        clean_threshold = post_processing_threshold(simple_threshold)

        mask_list.append(clean_threshold)

    return mask_list

def colorize_connected_components(labels):
    label_hue = np.uint8(179*labels/np.max(labels))
    blank_ch = 255*np.ones_like(label_hue)
    labeled_img = cv.merge([label_hue, blank_ch, blank_ch])

    labeled_img = cv.cvtColor(labeled_img, cv.COLOR_HSV2BGR)

    labeled_img[label_hue==0] = 0

"""# Main"""

# List with all the DAY folders
path_sampling_lst = sorted(Path(CONFIG_INPUT).glob("*"))
sample_list = get_samples_name_list(path_sampling_lst)
csv_table = []

# For each sample on the first folder, find it in every other folder.
for sample in tqdm(sample_list):

    # List of this SAMPLE image for every folder.
    sample_image_lst = get_sample_images(sample.name, path_sampling_lst)

    # Segmentation pipeline -
    segmented_sample_list = segmentation_method(sample_image_lst)
    spheroid_mask_list, spheroid_area_list = get_spheroid_mask_from_thresh(segmented_sample_list)

    save_sample_segmentation_render(CONFIG_INPUT,
                                    sample,
                                    spheroid_mask_list,
                                    spheroid_area_list)

    csv_table.append(spheroid_area_list)

save_segmentation_csv(CONFIG_INPUT, sample_list, csv_table)

def replace_nan_with_mean(csv_table):
    """
    Replaces NaN values in a CSV table with the mean of their pairs, ignoring the first row and ignoring the first column.

    Args:
        csv_table: A list of lists representing the CSV table.

    Returns:
        A new list of lists with NaN values replaced by the mean of their pairs.
    """

    # Get the number of rows and columns in the table.
    num_rows = len(csv_table)
    num_cols = len(csv_table[0])

    # Replaces NaN with zeros
    for i in range(num_rows):
        # Iterate over the columns, starting from the second column.
        for j in range(num_cols):
            # If the value is NaN, replace it with the mean of its pairs.
            if csv_table[i][j] == 'NaN':
                csv_table[i][j] = 0

    # Iterate over the rows, starting from the second row.
    for i in range(num_rows):
        # Iterate over the columns, starting from the second column.
        for j in range(num_cols):
            # If the value is NaN, replace it with the mean of its pairs.
            if csv_table[i][j] == 0:
                # Get the values of the pairs.

                if j == 0:
                    mean = csv_table[i][j+1] / 2
                elif j == num_cols-1:
                    mean = csv_table[i][j-1] / 2
                else:
                    pair1 = csv_table[i][j-1]
                    pair2 = csv_table[i][j+1]

                    # Calculate the mean of the pairs.
                    mean = (float(pair1) + float(pair2)) / 2

                csv_table[i][j] = mean

    return csv_table

# new_csv_table = replace_nan_with_mean(csv_table)

"""# Statistical Analysis

## Statistics related functions
"""

# To define the dataframe used to do estatistical analysis

df = pd.read_csv(CONFIG_INPUT + "/areas.csv")

"""## Normal distribution"""

def is_column_normal(df, column):
    '''
        Checks whether a column of the dataframe follows the normal distribution.
        Uses the Shapiro test, and threshold of p-value < 0.05

        Returns:
            - True if distribution is normal, or False otherwise.
    '''

    result = stats.shapiro(df[column])

    if result.pvalue < 0.05:
        return False
    else:
        return True

# To define experimental conditions

interval_lst = df["time"].unique()

for it in interval_lst:

    print(f'Interval {it}')

    # Get control samples
    compound_prefix = 'control_'
    control_sample_lst = df.query(f'time == {it} and sample.str.contains(@compound_prefix)')
    print(f'\tFound {len(control_sample_lst)} samples in CONTROL')

    # Get samples for each treatment
    for tto in range(TTT_COUNT):
        compound_prefix = f'test_{str(tto).zfill(2)}_'
        compound_sample_lst = df.query(f'time == {it} and sample.str.contains(@compound_prefix)')
        print(f'\tFound {len(compound_sample_lst)} samples in treatment {compound_prefix[:-1].upper()}')

"""## Remove outliers from "area" column"""

def remove_outliers(df, column):
    '''
        Removes outliers from a specific column of the dataframe.
        Uses the interquantile range method to remove outliers.
    '''

    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    filter = (df[column] >= Q1 - 1.5 * IQR) & (df[column] <= Q3 + 1.5 * IQR)
    new_df = df.loc[filter]

    # Debug
    # TODO : Remove this
    # print('BEFORE')
    # print(df)
    # print('AFTER')
    # print(new_df)

    return new_df

"""## T-Test"""

def run_ttest(df_one, df_two):
    '''
        Executes the T-Test analysis on distributions df_one VS df_two
        Significance means p-value < 0.05
    '''

    result = stats.ttest_ind(df_one, df_two)

    print(f'\tT-Test CONTROL vs {compound_prefix[:-1].upper()}')
    print(f'\t\tP-value:{result.pvalue:.5f} Significance: ', end='')

    if result.pvalue < 0.0001:
        print('YES', '*' * 4)
    elif result.pvalue < 0.001:
        print('YES', '*' * 3)
    elif result.pvalue < 0.01:
        print('YES', '*' * 2)
    elif result.pvalue < 0.05:
        print('YES', '*' * 1)
    else:
        print('NO')

    print(f'\t\tMean CONTROL: {np.mean(df_one):.2f}\n\t\tMean {compound_prefix[:-1].upper()}: {np.mean(df_two):.2f}')

"""## ANOVA test (TALVEZ COLOCAR O TUKEY AQUI NESTE BLOCO)"""

def run_anova(control_df, ttos_lst):
    # TODO : This has to be manually set to the number of compounds being tested
    result = stats.f_oneway(control_df["area"], ttos_lst[0]["area"], ttos_lst[1]["area"], ttos_lst[2]["area"])
    # result = stats.f_oneway(control_df["area"], *ttos_lst)

    print(f'\tANOVA for THIS interval')
    print(f'\t\tP-value:{result.pvalue:.5f} Significance: ', end='')

    if result.pvalue < 0.0001:
        print('YES', '*' * 4)
    elif result.pvalue < 0.001:
        print('YES', '*' * 3)
    elif result.pvalue < 0.01:
        print('YES', '*' * 2)
    elif result.pvalue < 0.05:
        print('YES', '*' * 1)
    else:
        print('NO')

    print(f'\t\tMean CONTROL: {np.mean(control_df["area"]):.2f}')

    for i, tto_df in enumerate(ttos_lst):
        compound_prefix = f'test_{str(i).zfill(2)}_'
        # print(tto_df) # TODO : Delete this

        print(f'\t\tMean {compound_prefix[:-1].upper()}: {np.mean(tto_df["area"]):.2f}')

interval_lst = df["time"].unique()

graph_df = pd.DataFrame()

for it in interval_lst:

    print(f'\nInterval {it}', '=' * 60)

    # Get control samples
    compound_prefix = 'control_'
    control_sample_lst = df.query(f'time == {it} and sample.str.contains(@compound_prefix)')
    print(f'\tFound {len(control_sample_lst)} samples in CONTROL')

    control_sample_lst = remove_outliers(control_sample_lst, 'area')
    if not is_column_normal(control_sample_lst, 'area'):
        # no_outliers = remove_outliers(control_sample_lst, 'area')
        # control_sample_lst = no_outliers
        print('\tWARNING : Distribution NOT normal')
        # print(f'\tNew shapiro: {stats.shapiro(control_sample_lst["area"]).pvalue}')
    print()

    graph_df = pd.concat([graph_df, control_sample_lst], ignore_index=True)

    ttos_this_interval = []
    # Get samples for each treatment
    for tto in range(TTT_COUNT):
        compound_prefix = f'test_{str(tto).zfill(2)}_'
        compound_sample_lst = df.query(f'time == {it} and sample.str.contains(@compound_prefix)')
        print(f'\tFound {len(compound_sample_lst)} samples in treatment {compound_prefix[:-1].upper()}')

        compound_sample_lst = remove_outliers(compound_sample_lst, 'area')
        if not is_column_normal(compound_sample_lst, 'area'):
            # no_outliers = remove_outliers(compound_sample_lst, 'area')
            # compound_sample_lst = no_outliers
            print('\tWARNING : Distribution NOT normal')
            # print(f'\tNew shapiro: {stats.shapiro(compound_sample_lst["area"]).pvalue}')

        run_ttest(control_sample_lst['area'], compound_sample_lst['area'])
        ttos_this_interval.append(compound_sample_lst)

        graph_df = pd.concat([graph_df, compound_sample_lst], ignore_index=True)

        print()
    print(ttos_this_interval)
    run_anova(control_sample_lst, ttos_this_interval)
    res = tukey_hsd(control_sample_lst['area'], ttos_this_interval[0]["area"], ttos_this_interval[1]["area"])
    print('ARRUMAR ISSO, TUKEY:')
    print(res)

print(len(graph_df), len(df))
print(graph_df)

# import seaborn as sns
# import matplotlib.pyplot as plt

sns.set_theme(style="ticks", palette="pastel")

# Initialize the figure with a logarithmic x axis
f, ax = plt.subplots(figsize=(11, 6))

# Plot the orbital period with horizontal boxes
sns.boxplot(
    data=df,
    x="time",
    y="area",
    hue="experiment",
    whis=[0, 100],
    width=.6
)

# Add in points to show each observation
sns.stripplot(data=df, x="time", y="area", hue="experiment", size=3.5, color=".6")

# Tweak the visual presentation
ax.xaxis.grid(True)
ax.set(ylabel="Area uM^2")
ax.set(xlabel="Day")
sns.despine(trim=True, left=True)

"""## Conditions names and times correction"""

#Correction of experimental names and times conditions

graph_df = graph_df.replace({'control': 'Control', 'test_00': 'Vemurafenib 10µM', 'test_01': 'Violacein 2.5µM', 'test_02': 'Violacein 5µM',})
graph_df.loc[graph_df["time"] == 0, "time"] = "0"
graph_df.loc[graph_df["time"] == 1, "time"] = "2"
graph_df.loc[graph_df["time"] == 2, "time"] = "4"
graph_df.loc[graph_df["time"] == 3, "time"] = "6"
graph_df.loc[graph_df["time"] == 4, "time"] = "8"
graph_df.loc[graph_df["time"] == 5, "time"] = "10"

# print(list(combinations(df["experiment"].unique(), r=1)))
# print(df["experiment"].unique())
# comb_pairs = 1


# aux = list(combinations(df["experiment"].unique(), r=2))

pair = []
experiments = graph_df["experiment"].unique()
n_experiments = len(experiments)

# print(graph_df)
# print(type(graph_df))
#TESTE COM OS NOMES AQUI - BIA - DEU CERTO ALTERAR OS NOMES DAS LEGENDAS AQUI
#graph_df = graph_df.replace({'1': '2', '2': '4', '3': '6', '4': '8', '5': '10',})
# print(type(graph_df))
# print(graph_df)
#graph_df = graph_df.loc[graph_df["time"] == 2, "time"] = "Grupo_A"
# print(graph_df.loc[graph_df["time"] == 2])

# (graph_df.loc[graph_df["time"] == 0])["time"] = "Grupo_A"
# print(graph_df.head)
#tmp["time"] = "Grupo_A"
# print(tmp)
# break
# T-Test combinations
n_intervals = graph_df["time"].unique()
for inter in n_intervals:

    # print(f'\nInterval {inter}')
    for i in range(1, n_experiments):
        pair.append(((inter, experiments[0]), (inter, experiments[i])))

# print(pair)

"""## Plot settings"""

# import seaborn as sns
# import matplotlib.pyplot as plt
# import plotly.express as px
# import plotly
# pd.options.plotting.backend = 'plotly'


# sns.set_theme(style="white", palette="pastel")

# # Initialize the figure with a logarithmic x axis
# f, ax = plt.subplots(figsize=(15, 12))

# # Plot the orbital period with horizontal boxes
# sns.boxplot(
#     data=graph_df,
#     x="time",
#     y="area",
#     hue="experiment",
#     whis=[0, 100],
#     width=.6,
#     linewidth=0.9,
# )

# # O SEGREDO ESTÁ EM ACERTAR O VALOR DE x AQUI (TESTE-T)
# annotator = Annotator(ax, data=graph_df, pairs=pair, x='time', y='area', hue='experiment')
# annotator.configure(test="t-test_ind", loc='inside')
# annotator.apply_and_annotate()
# ax.legend(loc='upper left')


# # Add in points to show each observation
# # sns.stripplot(data=df,
# #               x="time",
# #               y="area",
# #               hue="experiment",
# #               size=3.5,
# #               )

# # Tweak the visual presentation
# # ax.xaxis.grid(True)
# ax.set(ylabel="Area (µm²)")
# ax.set(xlabel="Days")
# sns.despine(trim=True, left=False)

#import seaborn as sns - CÓPIA PARA TESTE DO PLOTLY
import matplotlib.pyplot as plt
import plotly.express as px
import plotly
pd.options.plotting.backend = 'plotly'
import plotly.graph_objects as go


plot = px.box(graph_df, x="time", y="area", points= 'all', color="experiment")

plot.update_layout(
    height=600,
    width=1500,
    legend_title=None,
      font=dict(
      size=15,
    ),

   yaxis=dict(
      title="Area (µm²)",
      showgrid=True,
      gridcolor='rgb(255, 255, 255)',
    ),

   xaxis=dict(
       title="Days of treatment",
        ),

      paper_bgcolor='rgb(255, 255, 255)',
      plot_bgcolor='rgb(233, 233, 233)',
      showlegend=True,
)